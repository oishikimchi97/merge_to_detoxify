defaults:
  - trainer: adamw-descent

name: "llama2-7b"
hf_key: "meta-llama/Llama-2-7b-chat-hf"

args:
  use_flash_attention_2: true
  torch_dtype:
    _target_: hydra.utils.get_object
    path: torch.bfloat16

trainer:
  args:
    bf16: true
    bf16_full_eval: true
    deepspeed: "config/ds_config.json"
    gradient_checkpointing: true
    gradient_accumulation_steps: 2
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
